{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneline_log(text):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "\n",
    "    def linear(self, x, w, b):\n",
    "\n",
    "        return w * x.T + b\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "\n",
    "        x = np.clip(x, -709.78, 709.78)\n",
    "\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x, w, b):\n",
    "        x = np.mat(x)\n",
    "        w = np.mat(w)\n",
    "        # print(f'x shape: {np.shape(x)}')\n",
    "        # print(f'w shape: {np.shape(w)}')\n",
    "        net_input = self.linear(x, w, b)\n",
    "        # print(f'net_input: {net_input}')\n",
    "        y_estimate = self.sigmoid(net_input)\n",
    "\n",
    "        return y_estimate\n",
    "\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropy\n",
    "class BinaryCrossEntropy():\n",
    "    def __init__(self):\n",
    "        super(BinaryCrossEntropy, self).__init__()\n",
    "    \n",
    "    def cross_entropy(self, y_pred, target):\n",
    "        x = target*np.log(y_pred) + (1-target)*np.log(1-y_pred)\n",
    "\n",
    "        return -(np.mean(x))\n",
    "\n",
    "    def forward(self, y_pred, target):\n",
    "\n",
    "        return self.cross_entropy(y_pred, target)\n",
    "criterion = BinaryCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Function: Cross-entropy loss\n",
    "# used to calculate the loss of estimate\n",
    "# a: estimate value of y\n",
    "# y: true value of y\n",
    "def calculate_cross_entropy(y, a):\n",
    "    return -np.nan_to_num(np.multiply(y, np.log(a)) + np.multiply((1-y), np.log(1-a))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random weight for each layer\n",
    "# number of weight = input * neuron of next layer\n",
    "\n",
    "# return value is 2D array of weight w_ji \n",
    "# w_ji means for the j neuron, the weight of input i\n",
    "random_scalar = 100\n",
    "def generate_layer_weight(seed, neuron, input):\n",
    "    np.random.seed(seed) # set seed for weight random\n",
    "    # w_ji 其中 j 对应 neuron, i 对应 input，所以 reshape 也同样按照如此进行\n",
    "    weight = np.random.randn(neuron,input) / random_scalar\n",
    "    # weight = np.zeros((neuron,input))\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random bias for each layer\n",
    "# number of bias = neuron of layer\n",
    "\n",
    "# return value is vector of bias\n",
    "def generate_layer_bias(seed, neuron):\n",
    "    np.random.seed(seed) # set seed for bias random\n",
    "    bias = np.random.randn(neuron, 1) / random_scalar\n",
    "    # bias = np.zeros((neuron,1))\n",
    "    return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(value):\n",
    "    if value == 9:\n",
    "        return np.array([0,0,0,1])\n",
    "    elif value == 8:\n",
    "        return np.array([0,0,1,0])\n",
    "    elif value == 3:\n",
    "        return np.array([0,1,0,0])\n",
    "    elif value == 0:\n",
    "        return np.array([1,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('================== Start ==================')\n",
    "pd_train_origin = pd.read_csv('data/lab3_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保留 label 为 0、3、8、9 的 data\n",
    "# 用 drop() 的方法，参见: https://www.cnblogs.com/everfight/p/pandas_condition_remove.html\n",
    "pd_train_origin = pd_train_origin[(pd_train_origin.label == 0) \n",
    "                                | (pd_train_origin.label == 3) \n",
    "                                | (pd_train_origin.label == 8) \n",
    "                                | (pd_train_origin.label == 9) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train = pd_train_origin.sample(frac=0.8, random_state=2)\n",
    "pd_validate = pd_train_origin.drop(index=pd_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3200 entries, 1 to 15994\n",
      "Columns: 785 entries, label to pixel784\n",
      "dtypes: int64(785)\n",
      "memory usage: 19.2 MB\n"
     ]
    }
   ],
   "source": [
    "pd_validate.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12800 entries, 464 to 13599\n",
      "Columns: 785 entries, label to pixel784\n",
      "dtypes: int64(785)\n",
      "memory usage: 76.8 MB\n"
     ]
    }
   ],
   "source": [
    "pd_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = pd_train.drop(['label'], axis=1)\n",
    "train_feature = train_feature / 255\n",
    "train_target = pd.DataFrame(pd_train.label)\n",
    "train_feature = np.array(train_feature)\n",
    "train_target = np.array(train_target)\n",
    "\n",
    "validate_feature = pd_validate.drop(['label'], axis=1)\n",
    "validate_feature = validate_feature / 255\n",
    "validate_target = pd.DataFrame(pd_validate.label)\n",
    "validate_feature = np.array(validate_feature)\n",
    "validate_target = np.array(validate_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一共 len(train_feature) 笔 data\n",
    "# 每一笔 data 有 784 个 pixel\n",
    "\n",
    "w = {} # weight of layers\n",
    "b = {} # bias of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[784, 36, 4]\n",
      "layer 0, weight shape: (36, 784)\n",
      "layer 0, bias shape: (36, 1)\n",
      "layer 1, weight shape: (4, 36)\n",
      "layer 1, bias shape: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# weight for hidden layers\n",
    "layer_neuron = [36, 4]\n",
    "all_layer_neuron = [784]\n",
    "all_layer_neuron.extend(layer_neuron)\n",
    "print(all_layer_neuron)\n",
    "for i, neuron in enumerate(layer_neuron):\n",
    "    input_size = 784 if i == 0 else layer_neuron[i-1]\n",
    "    w[i+1] = generate_layer_weight(seed=2, neuron=neuron, input=input_size)\n",
    "    print(f'layer {i}, weight shape: {np.shape(w[i+1])}')\n",
    "    # print(w[i+1])\n",
    "    b[i+1] = generate_layer_bias(seed=2, neuron=neuron)\n",
    "    print(f'layer {i}, bias shape: {np.shape(b[i+1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 1000\n",
    "learning_rate = 0.002\n",
    "validate_size = len(validate_target)\n",
    "train_size = len(train_target)\n",
    "best_w = {} # weight of layers\n",
    "best_b = {} # bias of layers\n",
    "best_epoch = 0\n",
    "best_train_loss = 0\n",
    "best_validate_loss = 0\n",
    "max_train_acc = 0\n",
    "max_validate_acc = 0\n",
    "overfit_threshold = 0.005 # 如果 acc 比 max_validate_acc 小 overfit_threshold 的话\n",
    "is_overfit = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: max_validate_acc = 0.9421875, train_acc = 0.85484375, train_loss = 0.21342946024334467, validate_loss = 0.09249141391621941, validate_acc = 0.9421875\n",
      "epoch 2: max_validate_acc = 0.9571875, train_acc = 0.945703125, train_loss = 0.08391462499892471, validate_loss = 0.07027201903348242, validate_acc = 0.9571875\n",
      "epoch 3: max_validate_acc = 0.958125, train_acc = 0.953046875, train_loss = 0.07259221638436314, validate_loss = 0.06426378977088594, validate_acc = 0.958125\n",
      "epoch 4: max_validate_acc = 0.958125, train_acc = 0.955703125, train_loss = 0.06771541118631834, validate_loss = 0.06138263450463298, validate_acc = 0.956875\n",
      "epoch 5: max_validate_acc = 0.9584375, train_acc = 0.956875, train_loss = 0.06463136198160034, validate_loss = 0.059706928493615384, validate_acc = 0.9584375\n",
      "epoch 6: max_validate_acc = 0.959375, train_acc = 0.958125, train_loss = 0.06238235542969355, validate_loss = 0.05859882458477848, validate_acc = 0.959375\n",
      "epoch 7: max_validate_acc = 0.9596875, train_acc = 0.959375, train_loss = 0.060604670563023585, validate_loss = 0.0578149379852258, validate_acc = 0.9596875\n",
      "epoch 8: max_validate_acc = 0.9603125, train_acc = 0.960078125, train_loss = 0.05913471385410346, validate_loss = 0.0572581021729014, validate_acc = 0.9603125\n",
      "epoch 9: max_validate_acc = 0.9609375, train_acc = 0.959921875, train_loss = 0.05788258515476253, validate_loss = 0.05686081977078273, validate_acc = 0.9609375\n",
      "epoch 10: max_validate_acc = 0.9609375, train_acc = 0.96046875, train_loss = 0.056794969792255276, validate_loss = 0.056567555015943564, validate_acc = 0.9609375\n",
      "epoch 11: max_validate_acc = 0.9609375, train_acc = 0.961171875, train_loss = 0.05584127613118958, validate_loss = 0.05635639933922278, validate_acc = 0.960625\n",
      "epoch 12: max_validate_acc = 0.96125, train_acc = 0.9625, train_loss = 0.05499606719095903, validate_loss = 0.05621690692333606, validate_acc = 0.96125\n",
      "epoch 13: max_validate_acc = 0.961875, train_acc = 0.962734375, train_loss = 0.05423919902075083, validate_loss = 0.05613550662581566, validate_acc = 0.961875\n",
      "epoch 14: max_validate_acc = 0.961875, train_acc = 0.963203125, train_loss = 0.053553903716295884, validate_loss = 0.05609976820588475, validate_acc = 0.96125\n",
      "epoch 15: max_validate_acc = 0.961875, train_acc = 0.963828125, train_loss = 0.05292551414119892, validate_loss = 0.05610006441008172, validate_acc = 0.9615625\n",
      "epoch 16: max_validate_acc = 0.9621875, train_acc = 0.96421875, train_loss = 0.05234260272626451, validate_loss = 0.05612800657849475, validate_acc = 0.9621875\n",
      "epoch 17: max_validate_acc = 0.9621875, train_acc = 0.964609375, train_loss = 0.05179670244215848, validate_loss = 0.056175398481622583, validate_acc = 0.961875\n",
      "epoch 18: max_validate_acc = 0.9621875, train_acc = 0.96515625, train_loss = 0.051281275990528695, validate_loss = 0.05623404073397806, validate_acc = 0.960625\n",
      "epoch 19: max_validate_acc = 0.9621875, train_acc = 0.96546875, train_loss = 0.05079108370750642, validate_loss = 0.056296304899417386, validate_acc = 0.9609375\n",
      "epoch 20: max_validate_acc = 0.9621875, train_acc = 0.9659375, train_loss = 0.050322006062773206, validate_loss = 0.05635621706686134, validate_acc = 0.9609375\n",
      "epoch 21: max_validate_acc = 0.9621875, train_acc = 0.96609375, train_loss = 0.04987104517703164, validate_loss = 0.05641048453591238, validate_acc = 0.9603125\n",
      "epoch 22: max_validate_acc = 0.9621875, train_acc = 0.966328125, train_loss = 0.04943625709388101, validate_loss = 0.0564589062625376, validate_acc = 0.96\n",
      "epoch 23: max_validate_acc = 0.9621875, train_acc = 0.966640625, train_loss = 0.04901652429080744, validate_loss = 0.056503750637014495, validate_acc = 0.960625\n",
      "epoch 24: max_validate_acc = 0.9621875, train_acc = 0.9671875, train_loss = 0.04861127494361797, validate_loss = 0.056548162867171986, validate_acc = 0.96\n",
      "epoch 25: max_validate_acc = 0.9621875, train_acc = 0.967265625, train_loss = 0.04822031814466745, validate_loss = 0.056594563935921965, validate_acc = 0.959375\n",
      "epoch 26: max_validate_acc = 0.9621875, train_acc = 0.96734375, train_loss = 0.04784388587179924, validate_loss = 0.05664417111414207, validate_acc = 0.959375\n",
      "epoch 27: max_validate_acc = 0.9621875, train_acc = 0.967734375, train_loss = 0.04748280674228959, validate_loss = 0.056697734030352345, validate_acc = 0.959375\n",
      "epoch 28: max_validate_acc = 0.9621875, train_acc = 0.96796875, train_loss = 0.04713851564730891, validate_loss = 0.05675633465349232, validate_acc = 0.9590625\n",
      "epoch 29: max_validate_acc = 0.9621875, train_acc = 0.968203125, train_loss = 0.04681269437314504, validate_loss = 0.05682110458078902, validate_acc = 0.9590625\n",
      "epoch 30: max_validate_acc = 0.9621875, train_acc = 0.96828125, train_loss = 0.04650684225942497, validate_loss = 0.056892443029069435, validate_acc = 0.9590625\n",
      "epoch 31: max_validate_acc = 0.9621875, train_acc = 0.96828125, train_loss = 0.0462221729731812, validate_loss = 0.05696996680619864, validate_acc = 0.9590625\n",
      "epoch 32: max_validate_acc = 0.9621875, train_acc = 0.96890625, train_loss = 0.04595975144430423, validate_loss = 0.05705324213616888, validate_acc = 0.9590625\n",
      "epoch 33: max_validate_acc = 0.9621875, train_acc = 0.969375, train_loss = 0.04572051491805573, validate_loss = 0.057142955691354726, validate_acc = 0.9590625\n",
      "epoch 34: max_validate_acc = 0.9621875, train_acc = 0.969609375, train_loss = 0.045505043289307975, validate_loss = 0.05724246886825218, validate_acc = 0.95875\n",
      "epoch 35: max_validate_acc = 0.9621875, train_acc = 0.969765625, train_loss = 0.04531347106387914, validate_loss = 0.05735929860205191, validate_acc = 0.9584375\n",
      "epoch 36: max_validate_acc = 0.9621875, train_acc = 0.97, train_loss = 0.04514602755417389, validate_loss = 0.05750440683173357, validate_acc = 0.9578125\n",
      "epoch 37: max_validate_acc = 0.9621875, train_acc = 0.97, train_loss = 0.045002468205250994, validate_loss = 0.05768584152262141, validate_acc = 0.958125\n",
      "epoch 38"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_num):\n",
    "    train_loss_sum = 0\n",
    "    oneline_log(f'epoch {epoch + 1}')\n",
    "    train_acc_count = 0\n",
    "    for i, feature_data in enumerate(train_feature):\n",
    "        # 第 i 笔 data 的 feature\n",
    "        a = {} # output of layers\n",
    "        error = {} # error of layers\n",
    "        a[0] = feature_data\n",
    "\n",
    "        # Forward\n",
    "        for layer, neuron in enumerate(layer_neuron):\n",
    "            output = model.forward(a[layer], w[layer+1], b[layer+1])            \n",
    "            a[layer+1] = np.array(output.reshape(1,-1))[0]\n",
    "\n",
    "        # Backward\n",
    "        y = one_hot(train_target[i][0])\n",
    "        loss_estimate = a[len(layer_neuron)]\n",
    "        train_loss_sum += criterion.forward(loss_estimate, y)\n",
    "        arr = a[len(layer_neuron)]\n",
    "\n",
    "        for i, data in enumerate(y):\n",
    "            if data == 1 and arr[i] == np.max(arr):\n",
    "                train_acc_count += 1\n",
    "        \n",
    "        error[len(layer_neuron)] = np.mat(loss_estimate - y).T\n",
    "\n",
    "        for layer in range(len(layer_neuron) - 1, -1, -1):\n",
    "            # print(f'layer: {layer}')\n",
    "            left = np.mat(w[layer+1]).T\n",
    "            right = error[layer+1] * np.dot( a[layer], 1-a[layer])\n",
    "            error[layer] = np.dot(left , right)\n",
    "            # print(f'error {layer}: {error[layer]}')\n",
    "\n",
    "        # Update parameter\n",
    "        for layer in range(1, len(layer_neuron)+1):\n",
    "            dw = np.dot(error[layer] , np.mat(a[layer-1]))\n",
    "            w[layer] -= learning_rate * dw\n",
    "            b[layer] -= learning_rate * error[layer]\n",
    "\n",
    "    train_loss = train_loss_sum / train_size\n",
    "    \n",
    "    if (epoch+1) % 1 == 0:\n",
    "        validate_acc_count = 0\n",
    "        validate_loss_sum = 0\n",
    "        \n",
    "        for i, feature_data in enumerate(validate_feature):\n",
    "            a = {} # output of layers\n",
    "            error = {} # error of layers\n",
    "            a[0] = feature_data\n",
    "\n",
    "            # Forward\n",
    "            for layer, neuron in enumerate(layer_neuron):\n",
    "                output = model.forward(a[layer], w[layer+1], b[layer+1])            \n",
    "                a[layer+1] = np.array(output.reshape(1,-1))[0]\n",
    "\n",
    "            arr = a[len(layer_neuron)]\n",
    "\n",
    "            y = one_hot(validate_target[i][0])\n",
    "         \n",
    "            for i, data in enumerate(y):\n",
    "                if data == 1 and arr[i] == np.max(arr):\n",
    "                    validate_acc_count += 1\n",
    "            # Backward\n",
    "            loss_estimate = a[len(layer_neuron)]\n",
    "            validate_loss_sum += criterion.forward(loss_estimate, y)\n",
    "        \n",
    "        validate_loss = validate_loss_sum / validate_size\n",
    "        if max_train_acc < train_acc_count:\n",
    "            max_train_acc = train_acc_count\n",
    "            best_train_loss = train_loss\n",
    "        if max_validate_acc < validate_acc_count:\n",
    "            max_validate_acc = validate_acc_count\n",
    "            best_validate_loss = validate_loss\n",
    "            best_b = b\n",
    "            best_w = w\n",
    "            best_epoch = epoch\n",
    "        oneline_log('')\n",
    "        print(f'epoch {epoch + 1}: max_validate_acc = {max_validate_acc/validate_size}, train_acc = {train_acc_count / train_size}, train_loss = {train_loss}, validate_loss = {validate_loss}, validate_acc = {validate_acc_count/validate_size}')\n",
    "    \n",
    "    if max_validate_acc/validate_size - validate_acc_count/validate_size > overfit_threshold and epoch > 100:\n",
    "        is_overfit = True\n",
    "        print('========== stop because overfitting ==========')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best result at epoch 112: learning_rate=0.002, neuron=[784, 784, 784, 784, 784, 784, 12, 4], validate_acc=0.9596875, train_acc=0.96296875\n"
     ]
    }
   ],
   "source": [
    "print(f'best result at epoch {best_epoch + 1}: learning_rate={learning_rate}, neuron={all_layer_neuron}, validate_acc={max_validate_acc/validate_size}, train_acc={max_train_acc/train_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_test_origin = pd.read_csv('data/lab3_test.csv')\n",
    "# pd_test_origin = pd_test_origin / 255\n",
    "pd_test_origin = np.array(pd_test_origin)\n",
    "pd_test_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24952/2701855867.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneuron\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_neuron\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_b\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 3"
     ]
    }
   ],
   "source": [
    "ans = []\n",
    "for i, feature_data in enumerate(pd_test_origin):\n",
    "    a = {} # output of layers\n",
    "    error = {} # error of layers\n",
    "    a[0] = feature_data\n",
    "\n",
    "    # Forward\n",
    "    for layer, neuron in enumerate(layer_neuron):\n",
    "        output = model.forward(a[layer], best_w[layer+1], best_b[layer+1])            \n",
    "        a[layer+1] = np.array(output.reshape(1,-1))[0]\n",
    "\n",
    "    arr = a[len(layer_neuron)]\n",
    "\n",
    "    for i, data in enumerate(arr):\n",
    "        if data == np.max(arr):\n",
    "            if i == 0:\n",
    "                ans.append(0)\n",
    "            elif i == 1:\n",
    "                ans.append(3)\n",
    "            elif i == 2:\n",
    "                ans.append(8)\n",
    "            elif i == 3:\n",
    "                ans.append(9)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ans = pd.DataFrame(ans)\n",
    "test_ans = test_ans.rename({0:'ans'},axis=1)\n",
    "test_ans.to_csv('test_ans.csv', index=None)\n",
    "test_ans"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1bd4997df8f250b7ce125c4f296e41cc30fc4467602168a8546b0db04b01c027"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
