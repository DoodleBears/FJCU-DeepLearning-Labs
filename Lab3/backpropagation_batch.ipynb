{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneline_log(text):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "\n",
    "    def linear(self, x, w, b):\n",
    "\n",
    "        return w * x.T + b\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "\n",
    "        x = np.clip(x, -709.78, 709.78)\n",
    "\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x, w, b):\n",
    "        x = np.mat(x)\n",
    "        w = np.mat(w)\n",
    "        # print(f'x shape: {np.shape(x)}')\n",
    "        # print(f'w shape: {np.shape(w)}')\n",
    "        net_input = self.linear(x, w, b)\n",
    "        # print(f'net_input: {net_input}')\n",
    "        y_estimate = self.sigmoid(net_input)\n",
    "\n",
    "        return y_estimate\n",
    "\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropy\n",
    "class BinaryCrossEntropy():\n",
    "    def __init__(self):\n",
    "        super(BinaryCrossEntropy, self).__init__()\n",
    "    \n",
    "    def cross_entropy(self, y_pred, target):\n",
    "        x = target*np.log(y_pred) + (1-target)*np.log(1-y_pred)\n",
    "\n",
    "        return -(np.mean(x))\n",
    "\n",
    "    def forward(self, y_pred, target):\n",
    "\n",
    "        return self.cross_entropy(y_pred, target)\n",
    "criterion = BinaryCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Function: Cross-entropy loss\n",
    "# used to calculate the loss of estimate\n",
    "# a: estimate value of y\n",
    "# y: true value of y\n",
    "def calculate_cross_entropy(y, a):\n",
    "    return -np.nan_to_num(np.multiply(y, np.log(a)) + np.multiply((1-y), np.log(1-a))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random weight for each layer\n",
    "# number of weight = input * neuron of next layer\n",
    "\n",
    "# return value is 2D array of weight w_ji \n",
    "# w_ji means for the j neuron, the weight of input i\n",
    "random_scalar = 100\n",
    "def generate_layer_weight(seed, neuron, input):\n",
    "    np.random.seed(seed) # set seed for weight random\n",
    "    # w_ji 其中 j 对应 neuron, i 对应 input，所以 reshape 也同样按照如此进行\n",
    "    weight = np.random.randn(neuron,input) / random_scalar\n",
    "    # weight = np.zeros((neuron,input))\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random bias for each layer\n",
    "# number of bias = neuron of layer\n",
    "\n",
    "# return value is vector of bias\n",
    "def generate_layer_bias(seed, neuron):\n",
    "    np.random.seed(seed) # set seed for bias random\n",
    "    bias = np.random.randn(neuron, 1) / random_scalar\n",
    "    # bias = np.zeros((neuron,1))\n",
    "    return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(value):\n",
    "    if value == 9:\n",
    "        return [0,0,0,1]\n",
    "    elif value == 8:\n",
    "        return [0,0,1,0]\n",
    "    elif value == 3:\n",
    "        return [0,1,0,0]\n",
    "    elif value == 0:\n",
    "        return [1,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('================== Start ==================')\n",
    "pd_train_origin = pd.read_csv('data/lab3_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保留 label 为 0、3、8、9 的 data\n",
    "# 用 drop() 的方法，参见: https://www.cnblogs.com/everfight/p/pandas_condition_remove.html\n",
    "pd_train_origin = pd_train_origin[(pd_train_origin.label == 0) \n",
    "                                | (pd_train_origin.label == 3) \n",
    "                                | (pd_train_origin.label == 8) \n",
    "                                | (pd_train_origin.label == 9) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train = pd_train_origin.sample(frac=0.8, random_state=2)\n",
    "pd_validate = pd_train_origin.drop(index=pd_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3200 entries, 1 to 15994\n",
      "Columns: 785 entries, label to pixel784\n",
      "dtypes: int64(785)\n",
      "memory usage: 19.2 MB\n"
     ]
    }
   ],
   "source": [
    "pd_validate.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12800 entries, 464 to 13599\n",
      "Columns: 785 entries, label to pixel784\n",
      "dtypes: int64(785)\n",
      "memory usage: 76.8 MB\n"
     ]
    }
   ],
   "source": [
    "pd_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = pd_train.drop(['label'], axis=1)\n",
    "train_feature = train_feature / 255\n",
    "train_target = pd.DataFrame(pd_train.label)\n",
    "train_feature = np.array(train_feature)\n",
    "train_target = np.array(train_target)\n",
    "\n",
    "validate_feature = pd_validate.drop(['label'], axis=1)\n",
    "validate_feature = validate_feature / 255\n",
    "validate_target = pd.DataFrame(pd_validate.label)\n",
    "validate_feature = np.array(validate_feature)\n",
    "validate_target = np.array(validate_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一共 len(train_feature) 笔 data\n",
    "# 每一笔 data 有 784 个 pixel\n",
    "\n",
    "w = {} # weight of layers\n",
    "b = {} # bias of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0, weight shape: (12, 784)\n",
      "layer 0, bias shape: (12, 1)\n",
      "layer 1, weight shape: (4, 12)\n",
      "layer 1, bias shape: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# weight for hidden layers\n",
    "layer_neuron = [12, 4]\n",
    "for i, neuron in enumerate(layer_neuron):\n",
    "    input_size = 784 if i == 0 else layer_neuron[i-1]\n",
    "    w[i+1] = generate_layer_weight(seed=2, neuron=neuron, input=input_size)\n",
    "    print(f'layer {i}, weight shape: {np.shape(w[i+1])}')\n",
    "    # print(w[i+1])\n",
    "    b[i+1] = generate_layer_bias(seed=2, neuron=neuron)\n",
    "    print(f'layer {i}, bias shape: {np.shape(b[i+1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 1000\n",
    "learning_rate = 0.002\n",
    "validate_size = len(validate_target)\n",
    "train_size = len(train_target)\n",
    "best_w = {} # weight of layers\n",
    "best_b = {} # bias of layers\n",
    "best_epoch = 0\n",
    "best_train_loss = 0\n",
    "best_validate_loss = 0\n",
    "max_train_acc = 0\n",
    "max_validate_acc = 0\n",
    "overfit_threshold = 0.005 # 如果 acc 比 max_validate_acc 小 overfit_threshold 的话\n",
    "is_overfit = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_12308/1354213844.py, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_12308/1354213844.py\"\u001b[1;36m, line \u001b[1;32m21\u001b[0m\n\u001b[1;33m    y.append(one_hot(data) - )\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_num):\n",
    "    train_loss_sum = 0\n",
    "    oneline_log(f'epoch {epoch + 1}')\n",
    "    train_acc_count = 0\n",
    " \n",
    "    a = {} # output of layers\n",
    "    error = {} # error of layers\n",
    "    print(np.shape(train_feature))\n",
    "    a[0] = train_feature\n",
    "\n",
    "    # Forward\n",
    "    for layer, neuron in enumerate(layer_neuron):\n",
    "        output = model.forward(a[layer], w[layer+1], b[layer+1])            \n",
    "        a[layer+1] = np.array(output.mean(axis=1).reshape(1,-1))\n",
    "        # print(np.shape(a[layer+1]))\n",
    "\n",
    "    print(a[len(layer_neuron)].shape)\n",
    "    # Backward\n",
    "    y = []\n",
    "    for i, data in enumerate(train_target):\n",
    "        y.append(one_hot(data))\n",
    "    print(y)\n",
    "    # loss_estimate = a[len(layer_neuron)]\n",
    "    # train_loss_sum += criterion.forward(loss_estimate, y)\n",
    "    # arr = a[len(layer_neuron)]\n",
    "\n",
    "    # for i, data in enumerate(y):\n",
    "    #     if data == 1 and arr[i] == np.max(arr):\n",
    "    #         train_acc_count += 1\n",
    "    \n",
    "    # error[len(layer_neuron)] = np.mat(loss_estimate - y).T\n",
    "\n",
    "    # for layer in range(len(layer_neuron) - 1, -1, -1):\n",
    "    #     # print(f'layer: {layer}')\n",
    "    #     left = np.mat(w[layer+1]).T\n",
    "    #     right = error[layer+1] * np.dot( a[layer], 1-a[layer])\n",
    "    #     error[layer] = np.dot(left , right)\n",
    "    #     # print(f'error {layer}: {error[layer]}')\n",
    "\n",
    "    # # Update parameter\n",
    "    # for layer in range(1, len(layer_neuron)+1):\n",
    "    #     dw = np.dot(error[layer] , np.mat(a[layer-1]))\n",
    "    #     w[layer] -= learning_rate * dw\n",
    "    #     b[layer] -= learning_rate * error[layer]\n",
    "\n",
    "    # train_loss = train_loss_sum / train_size\n",
    "    \n",
    "    # if (epoch+1) % 1 == 0:\n",
    "    #     validate_acc_count = 0\n",
    "    #     validate_loss_sum = 0\n",
    "        \n",
    "    #     for i, feature_data in enumerate(validate_feature):\n",
    "    #         a = {} # output of layers\n",
    "    #         error = {} # error of layers\n",
    "    #         a[0] = feature_data\n",
    "\n",
    "    #         # Forward\n",
    "    #         for layer, neuron in enumerate(layer_neuron):\n",
    "    #             output = model.forward(a[layer], w[layer+1], b[layer+1])            \n",
    "    #             a[layer+1] = np.array(output.reshape(1,-1))[0]\n",
    "\n",
    "    #         arr = a[len(layer_neuron)]\n",
    "\n",
    "    #         y = one_hot(validate_target[i][0])\n",
    "         \n",
    "    #         for i, data in enumerate(y):\n",
    "    #             if data == 1 and arr[i] == np.max(arr):\n",
    "    #                 validate_acc_count += 1\n",
    "    #         # Backward\n",
    "    #         loss_estimate = a[len(layer_neuron)]\n",
    "    #         validate_loss_sum += criterion.forward(loss_estimate, y)\n",
    "        \n",
    "    #     validate_loss = validate_loss_sum / validate_size\n",
    "    #     if max_train_acc < train_acc_count:\n",
    "    #         max_train_acc = train_acc_count\n",
    "    #         best_train_loss = train_loss\n",
    "    #     if max_validate_acc < validate_acc_count:\n",
    "    #         max_validate_acc = validate_acc_count\n",
    "    #         best_validate_loss = validate_loss\n",
    "    #         best_b = b\n",
    "    #         best_w = w\n",
    "    #         best_epoch = epoch\n",
    "    #     oneline_log('')\n",
    "    #     print(f'epoch {epoch + 1}: max_validate_acc = {max_validate_acc/validate_size}, train_acc = {train_acc_count / train_size}, train_loss = {train_loss}, validate_loss = {validate_loss}, validate_acc = {validate_acc_count/validate_size}')\n",
    "    \n",
    "    # if max_validate_acc/validate_size - validate_acc_count/validate_size > overfit_threshold and epoch > 100:\n",
    "    #     is_overfit = True\n",
    "    #     print('========== stop because overfitting ==========')\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting: layer=2, neuron=[12, 4]\n",
      "best result at epoch 1: validate_acc = 0.0, train_acc = 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f'setting: layer={len(layer_neuron)}, neuron={layer_neuron}')\n",
    "print(f'best result at epoch {best_epoch + 1}: validate_acc = {max_validate_acc/validate_size}, train_acc = {max_train_acc/train_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_test_origin = pd.read_csv('data/lab3_test.csv')\n",
    "# pd_test_origin = pd_test_origin / 255\n",
    "pd_test_origin = np.array(pd_test_origin)\n",
    "pd_test_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12308/2701855867.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneuron\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_neuron\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_b\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "ans = []\n",
    "for i, feature_data in enumerate(pd_test_origin):\n",
    "    a = {} # output of layers\n",
    "    error = {} # error of layers\n",
    "    a[0] = feature_data\n",
    "\n",
    "    # Forward\n",
    "    for layer, neuron in enumerate(layer_neuron):\n",
    "        output = model.forward(a[layer], best_w[layer+1], best_b[layer+1])            \n",
    "        a[layer+1] = np.array(output.reshape(1,-1))[0]\n",
    "\n",
    "    arr = a[len(layer_neuron)]\n",
    "\n",
    "    for i, data in enumerate(arr):\n",
    "        if data == np.max(arr):\n",
    "            if i == 0:\n",
    "                ans.append(0)\n",
    "            elif i == 1:\n",
    "                ans.append(3)\n",
    "            elif i == 2:\n",
    "                ans.append(8)\n",
    "            elif i == 3:\n",
    "                ans.append(9)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " ...]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ans = pd.DataFrame(ans)\n",
    "test_ans = test_ans.rename({0:'ans'},axis=1)\n",
    "test_ans.to_csv('test_ans.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1bd4997df8f250b7ce125c4f296e41cc30fc4467602168a8546b0db04b01c027"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
